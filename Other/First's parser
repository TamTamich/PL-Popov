# 5-ая попытка. Неудачна.
# from bs4 import BeautifulSoup
#import requests
#import csv
#import time
#import re
#
#HOST = 'https://vsuet.ru'
#URL = 'https://vsuet.ru/obuchenie/faculties'
#
#headers = {
#
#    'user-agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
#}
#
#res = requests.get(URL, headers=headers)
#soup = BeautifulSoup(res.text, 'lxml')
#
#
#name_facult = soup.find_all('a',class_='box-service__item')
#for x in name_facult:
#    name_faculties = HOST + x['href']
#    res = requests.get(name_faculties, headers=headers)
#    soup = BeautifulSoup(res.text, 'lxml')
#    # -
#    try:
#        name_facultiess = soup.find('h2',itemprop='headline').text
#        name_dekan = soup.find('div',class_='dept-info__h-name').text
#        # - 
#        name_kaf_block = soup.find('div',class_='col-lg-3 pr-lg-2 pl-lg-0')
#        name_kaf = name_kaf_block.find_all('a', text=re.compile('Кафедра'))
#        for x in name_kaf:
#            name_kafedras = x.text
#            link_kafedras = HOST + x['href']
#            
#            res = requests.get(link_kafedras, headers=headers)
#            soup = BeautifulSoup(res.text, 'lxml')
#            name_kafedrass = soup.find('h2',itemprop='headline').text
#            name_zaved_kaf = soup.find('div',class_='dept-info__h-name').text
#
#            link_kafedrasss = HOST + x['href'] + '/sotr'
#            
#            res = requests.get(link_kafedrasss, headers=headers)
#            soup = BeautifulSoup(res.text, 'lxml')
#            
#            million_sotr_name = soup.find('a',class_='person-name').text
#            million_sotr_name_job = soup.find('span',class_='field-value').text
#            print(f"{million_sotr_name}:{million_sotr_name_job}")                
#
#
#    except Exception as ex:
#        print(ex)
#       print('---Error---')
#    
#
#
# 6-ая попытка:

from bs4 import BeautifulSoup
import requests
import csv
import re

HOST = 'https://vsuet.ru'
URL = 'https://vsuet.ru/obuchenie/faculties'

HEADERS = {
     'user-agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
}

res = requests.get(URL,headers=HEADERS)
soup = BeautifulSoup(res.text,'lxml')

eight_faculties_universuty = soup.find_all('a',class_='box-service__item')
for faculties in eight_faculties_universuty:
     faculties_text = faculties.text
     faculties_links = HOST + faculties['href']
     # Снизу идёт все 8 факультетов
     res = requests.get(faculties_links,headers=HEADERS)
     soup = BeautifulSoup(res.text,'lxml')
    
     try:
          name_dekan = soup.find('div',class_='dept-info__h-name').text
          name_faculties = soup.find('h2',itemprop='headline').text
          
          four_or_more_kafedra_university_block = soup.find('div','col-lg-3 pr-lg-2 pl-lg-0')
          four_or_more_kafedra_university = four_or_more_kafedra_university_block.find_all('a',text=re.compile('Кафедра'))
          for kafedra in four_or_more_kafedra_university:
               kafedra_text = kafedra.text
               kafedra_links = HOST + kafedra['href'] + '/sotr'

               res = requests.get(kafedra_links,headers=HEADERS)
               soup = BeautifulSoup(res.text,'lxml')
               # cнизу идёт от 4 до N кол-во кафедров
               all_numbers_of_kafedra_block = soup.find('div',class_='article')
               all_nameteacher_of_kafedra = all_numbers_of_kafedra_block.find_all('a',class_='person-name')
               for z in all_nameteacher_of_kafedra:
                    z_text = z.text
               all_jobteacher_of_kafedra = all_numbers_of_kafedra_block.find_all('span',class_='field-value')
               for m in all_jobteacher_of_kafedra:
                    m_text = m.text
                    # Через костыли, обходы эникейщика и т.д. сделан полу-дохлый парсер, первые 4 попытки опубликую позже.
                    # Данный парсер не предназначен для использования в коммерческих, личных целях, а только для практики использования парсера. 
                    # Также метод сохранения данных не будет выложен на гитхаб.
                    # Все вопросы в тг: @badc0median
               
                    
     except Exception as ex:
        print(ex)
     
# 7 попытка
from bs4 import BeautifulSoup
import requests
import csv
import time
import re
import json

HOST = 'https://vsuet.ru'
URL = 'https://vsuet.ru/obuchenie/faculties'

HEADERS = {
      'user-agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
 }

res = requests.get(URL,headers=HEADERS)

soup = BeautifulSoup(res.text,'lxml')
four_faculties = soup.find_all('a',class_='box-service__item')
all_details_of_faculties = {}
for item in four_faculties:
      item_text = item.text
      item_hrefs = HOST + item['href']
      all_details_of_faculties[item_text] = item_hrefs

      
      # with open('all_details_of_faculties.json','w',encoding='utf-8') as file:
      #       json.dump(all_details_of_faculties,file,indent=4,ensure_ascii=False)

with open('all_details_of_faculties.json',encoding='utf-8') as file:
      all_categories = json.load(file)
      
      
      for category_name, category_href in all_categories.items():
            try:
                  res = requests.get(category_href,headers=HEADERS)
                  soup = BeautifulSoup(res.text,'lxml')
                  name_of_faculties = soup.find('h2',itemprop='headline').text
                  name_dekan = soup.find('div',class_='dept-info__h-name').text
                  block_of_contacts = soup.find_all('span',class_='line-contacts')
                  number = block_of_contacts[0].text
                  mail = block_of_contacts[1].text
                  address = block_of_contacts[2].text

                  block_kafedra = soup.find('div',class_='col-lg-3 pr-lg-2 pl-lg-0')
                  xxx_kafedra = block_kafedra.find_all('a',text=re.compile('Кафедра'))
                  all_details_of_kafedra = {}             
                  for item in xxx_kafedra:
                        item_text = item.text
                        item_hrefs = HOST + item['href']
                        
                        # На этом этапе json не мог отработать +15 кафедр, без него обойдемся
                        res = requests.get(item_hrefs,headers=HEADERS)
                        soup = BeautifulSoup(res.text,'lxml')
                        name_of_kafedra = soup.find('h2',itemprop='headline').text
                        name_zam_dekan = soup.find('div',class_='dept-info__h-name').text
                        block_of_contacts_zam = soup.find_all('span',class_='line-contacts')
                        number_zam = block_of_contacts[0].text
                        mail_zam = block_of_contacts[1].text
                        address_zam = block_of_contacts[2].text

                        item_hrefs_other = HOST + item['href'] + '/sotr'
                        res = requests.get(item_hrefs_other,headers=HEADERS)
                        soup = BeautifulSoup(res.text,'lxml')

                        block_1v_of_sotr = soup.find('form',id='adminForm')
                        name_sotr = block_1v_of_sotr.find('div',class_='col-md-10 staff__info').find_all('a',class_='person-name')
                        for name in name_sotr:
                              name_text = name.text
                        xxx_zzz = block_1v_of_sotr.find('div',class_='col-md-10 staff__info').find_all('span',class_='field-value')
                        doljhoct_of_sotr = xxx_zzz[1].text
                    # Почему-то парсер не может обработать N-ое количество большой информации, на следующий попытки или будет улучшен код в bs4 или будет перезаписан в bs4 + selenium
            except Exception as ex:
                  print(ex)
            


            
      








          
          
               
          

     



    



     

     



     
     
